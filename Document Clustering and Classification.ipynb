{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Automatic Document Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnr\\OneDrive\\Ross\\CSC478\\hw3\n"
     ]
    }
   ],
   "source": [
    "#cd OneDrive/Ross/CSC478/hw3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import math\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Create your own distance function that, instead of using Euclidean distance, uses Cosine similarity. This is the distance function you will use to pass to the kMeans function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distCos(x, D):\n",
    "    D_norm = np.linalg.norm(D)\n",
    "    x_norm = np.linalg.norm(x)\n",
    "    sim = np.dot(D,x)/(D_norm * x_norm)\n",
    "    dist = 1 - sim\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Load the data set [Note: the data matrix provided has terms as rows and documents as columns. Since you will be clustering documents, you'll need to take the transpose of this matrix so that your main data matrix is a document x term matrix. In Numpy, you may use the \".T\" operation to obtain the transpose.] Then, split the data set (the document x term matrix) and set aside 20% for later use (see below). Use the 80% segment for clustering in the next part. The 20% portion must be a random subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# create a list for the terms\n",
    "terms = []\n",
    "terms_list = open('terms.txt','r')\n",
    "\n",
    "for t in terms_list:\n",
    "    terms.append(t.strip())\n",
    "terms_list.close()\n",
    "\n",
    "# lod the data\n",
    "data = np.genfromtxt('matrix.txt',delimiter=',')\n",
    "# transpose the data so that it is in document x term format\n",
    "data = data.T\n",
    "\n",
    "#targets = pd.read_csv(\"classes.txt\",skiprows=0, sep=' ')\n",
    "targets = np.genfromtxt('classes.txt',skip_header=1,delimiter=' ')\n",
    "# delete the index column from targets\n",
    "targets = np.delete(targets, 0, 1) # 0th index, delete column (0) not row (1)\n",
    "\n",
    "# split the data into 80-20 train-test\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, targets, test_size=0.2, random_state=33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) As in the case of Assignment 2, transform the term-frequencies to tfxidf values. Be sure to maintain DF values for each of the terms in the dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find doucment frequencies for each term\n",
    "DF = np.array([(x_train!=0).sum(0)])\n",
    "NDocs = len(x_train[0])\n",
    "# Create a matrix with all entries = NDocs\n",
    "NMatrix=np.ones(np.shape(x_train), dtype=float)*NDocs\n",
    "# Convert each entry into IDF values\n",
    "# Note that IDF is only a function of the term, so all rows will be identical.\n",
    "DivM = np.divide(NMatrix, DF)\n",
    "IDF = np.log2(DivM)\n",
    "# Finally compute the TFxIDF values for each document-term entry\n",
    "DT_tfidf = x_train * IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Perform Kmeans clustering on the training data. Write a function to display the top N terms in each cluster along with the cluster DF values for each term and the size of the cluster. Cluster DF value for a term t in a cluster C is the percentage of docs in cluster C in which term t appears. Sort the terms in decreasing order of the DF percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import kMeans\n",
    "\n",
    "centroids, clusterAssment = kMeans.kMeans(x_train, 5, distMeas=distCos)\n",
    "#centroids, clusterAssment = kMeans.kMeans(DT_tfidf, 5, distMeas=distCos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n        \\n        print 'Number of documents in cluster: ', doc_count\\n        cluster_count += 1\\n\""
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def display_terms(clusterAssment, centroids, x_train, terms, k):\n",
    "    cluster_dict = {}\n",
    "    for c in clusterAssment:\n",
    "        if int(c[0]) in cluster_dict:\n",
    "            cluster_dict[int(c[0])] += 1\n",
    "        else:\n",
    "            cluster_dict[int(c[0])] = 1\n",
    "            \n",
    "    cluster_count = 0     \n",
    "    for cluster in cluster_dict:\n",
    "        ind = np.argpartition(centroids[cluster_count], -k)[-k:]\n",
    "        print 'Cluser ' + str(cluster_count) + ' results:\\n'\n",
    "        print 'Number of documents in cluster: ', cluster_dict[cluster_count]\n",
    "        print '\\tword\\tDF\\tPercentofDocs'\n",
    "        \n",
    "        word_count = 0\n",
    "        for i in ind:\n",
    "            term_found = np.array([(x_train[clusterAssment[:,0]==cluster_count][:,i]!=0).sum(0)])\n",
    "            print str(word_count) + '\\t' + terms[i] + '\\t' + str(term_found[0]) + '\\t' + str(term_found[0]*1.0 / cluster_dict[cluster_count]*1.0)\n",
    "            word_count +=1\n",
    "        print  \n",
    "        cluster_count += 1\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluser 0 results:\n",
      "\n",
      "Number of documents in cluster:  766\n",
      "\tword\tDF\tPercentofDocs\n",
      "0\trun\t141\t0.18407310705\n",
      "1\tsystem\t142\t0.185378590078\n",
      "2\twork\t180\t0.23498694517\n",
      "3\ton\t191\t0.249347258486\n",
      "4\tprogram\t135\t0.176240208877\n",
      "5\tsale\t210\t0.274151436031\n",
      "6\twindow\t294\t0.383812010444\n",
      "7\tdo\t152\t0.198433420366\n",
      "8\tsubject\t766\t1.0\n",
      "9\tfile\t158\t0.206266318538\n",
      "\n",
      "Cluser 1 results:\n",
      "\n",
      "Number of documents in cluster:  6\n",
      "\tword\tDF\tPercentofDocs\n",
      "0\ttm\t6\t1.0\n",
      "1\tasq\t6\t1.0\n",
      "2\tmq\t6\t1.0\n",
      "3\tqq\t6\t1.0\n",
      "4\tgiz\t6\t1.0\n",
      "5\tbxn\t6\t1.0\n",
      "6\tmax\t6\t1.0\n",
      "7\twm\t6\t1.0\n",
      "8\tax\t6\t1.0\n",
      "9\tpl\t6\t1.0\n",
      "\n",
      "Cluser 2 results:\n",
      "\n",
      "Number of documents in cluster:  346\n",
      "\tword\tDF\tPercentofDocs\n",
      "0\tbeliev\t144\t0.416184971098\n",
      "1\tknow\t152\t0.439306358382\n",
      "2\tsubject\t346\t1.0\n",
      "3\tchurch\t101\t0.291907514451\n",
      "4\tdb\t1\t0.0028901734104\n",
      "5\tpeopl\t164\t0.473988439306\n",
      "6\tchristian\t173\t0.5\n",
      "7\tjesu\t104\t0.300578034682\n",
      "8\tgod\t213\t0.615606936416\n",
      "9\ton\t208\t0.601156069364\n",
      "\n",
      "Cluser 3 results:\n",
      "\n",
      "Number of documents in cluster:  855\n",
      "\tword\tDF\tPercentofDocs\n",
      "0\tencrypt\t182\t0.212865497076\n",
      "1\tgo\t287\t0.33567251462\n",
      "2\tteam\t188\t0.219883040936\n",
      "3\tarticl\t417\t0.487719298246\n",
      "4\tget\t310\t0.362573099415\n",
      "5\tsubject\t855\t1.0\n",
      "6\twrite\t541\t0.632748538012\n",
      "7\tkei\t199\t0.232748538012\n",
      "8\tgame\t220\t0.25730994152\n",
      "9\ton\t372\t0.435087719298\n",
      "\n",
      "Cluser 4 results:\n",
      "\n",
      "Number of documents in cluster:  27\n",
      "\tword\tDF\tPercentofDocs\n",
      "0\tcal\t11\t0.407407407407\n",
      "1\tdet\t14\t0.518518518519\n",
      "2\tchi\t14\t0.518518518519\n",
      "3\tgm\t8\t0.296296296296\n",
      "4\tsp\t5\t0.185185185185\n",
      "5\tbo\t18\t0.666666666667\n",
      "6\tuw\t4\t0.148148148148\n",
      "7\tvs\t12\t0.444444444444\n",
      "8\tah\t3\t0.111111111111\n",
      "9\tcx\t5\t0.185185185185\n",
      "\n"
     ]
    }
   ],
   "source": [
    "display_terms(clusterAssment, centroids, x_train, terms, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Using the cluster assignments from Kmeans clustering, compare your 5 clusters to the 5 pre-assigned classes by computing the Completeness and Homogeneity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.659525865273\n",
      "0.454888372869\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import completeness_score, homogeneity_score\n",
    "\n",
    "y_train_int = y_train[:,0].astype(int)\n",
    "c = clusterAssment[:,0].astype(int)\n",
    "\n",
    "print completeness_score(y_train_int,c)\n",
    "print homogeneity_score(y_train_int,c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f) Using your cluster assignments as class labels, categorize each of the documents in the 20% set-aside data into each of the appropriate cluster. Your categorization should be based on Cosine similarity between each test document and each cluster centroids. Present your results in a separate file containing the obtained cluster label for each test document as well as Cosine similarities between each test document and each of the 5 clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def knn_search(x, D):\n",
    "    \"\"\" find nearest neighbour of data point test document x among cendtroids D \"\"\"\n",
    "    D_norm = np.array([np.linalg.norm(D[i]) for i in range(len(D))])\n",
    "    x_norm = np.linalg.norm(x)\n",
    "    sims = np.dot(D,x)/(D_norm * x_norm)\n",
    "    dists = 1 - sims\n",
    "    \n",
    "    idx = np.argsort(dists) # sorting\n",
    "    # return the index of nearest neighbor\n",
    "    return idx[:1], dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_file = open('Ross Claytor CSC478 HW3 Part 2f Categorizations.txt','w')\n",
    "\n",
    "for i in range(x_test.shape[0]):\n",
    "    label, dists = knn_search(x_test[i], centroids)\n",
    "    output_file.write('Test document ' + str(i+1) + '\\tCluster Prediction: ' + str(label[0]) + ' Distances: ' + str(dists) + '\\n')\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
